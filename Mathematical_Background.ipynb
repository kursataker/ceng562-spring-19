{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mathematical_Background.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kursataker/cng562-machine-learning-spring-19/blob/master/Mathematical_Background.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "YJdQZC7ENzEF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Calculus and Unconstrained Optimization\n"
      ]
    },
    {
      "metadata": {
        "id": "QufFsKFNJ5uA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Problem.** What is the number $t$ closest to given numbers $x_1, x_2, x_3$?\n",
        "\n",
        "**Solution.** Let $f(t)$ denote the sum of distances between the point $t$ and the points $x_1, x_2, x_3$. That is,\n",
        "\n",
        "$$ f(x) = (t-x_1)^2+(t-x_2)^2+(t-x_3)^2.$$\n",
        "\n",
        "**Analytic Approch.** Set $f'(t)=0$. Then $f'(t) = 2(x-x_1)+2(x-x_2)+2(x-x_3)=0.$\n",
        "\n",
        "Solving for $t$, we find $$t=\\frac{x_1+x_2+x_3}{3}.$$\n",
        "\n",
        "**Question.** *Is this really a minimum? How can you check?*\n",
        "\n",
        "**Computational approach.** \n",
        "1. Choose an initial randomly, $t_0$.\n",
        "2. Let $t_{n+1} = t_n - h \\cdot f'(t_n)$.\n",
        "3. Stop when the produce converges, that is when the distance between $t_n$ and $t_{n+1}$ becomes smaller than a prescribed value, $\\epsilon.$\n",
        "\n",
        "Here, $h$ is called the *learning rate* or *step size*.\n",
        "\n",
        "**Question.** *Did you really find a global minimum? How can you make sure?*\n",
        "\n",
        "\n",
        "**Comments.** \n",
        "\n",
        "1. The answer for the above problem with $N$ points $x_1, x_2, \\ldots, x_N$,\n",
        "would be similarly, the (arithmetic) mean\n",
        "\n",
        "$$t=\\frac{x_1+x_2+\\cdots+x_N}{N}.$$\n",
        "\n",
        "2. The function $f(t) = \\sum_{i=1}^N (t-x_i)^2$ is called the **sum of squared errors**. It is an example of a **loss** (or, **empirical risk**) function in ML literature. \n",
        "\n",
        "Loss functions are chosen according to the nature of the ML task.\n",
        "\n",
        "3. The function $f(t)$ is convex, that is the values of the function $f(t)$ always lie over the the tangen lines (spaces). Since it is convex, it has a global minimum, in this case the *arithmetic mean*.\n",
        "\n",
        "4. In higher dimensions, the derivative $f'(t)$ is replaced by the gradient $\\nabla f(t)$. For this reason, this method of computational minimization is called the *gradient descent algorithm*."
      ]
    },
    {
      "metadata": {
        "id": "EnUZREtSJHrJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Gradient Descent\n",
        "\n",
        "https://scipython.com/blog/visualizing-the-gradient-descent-method/\n",
        "\n",
        "## Animated\n",
        "\n",
        "https://github.com/Shathra/gradient-descent-demonstration\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "cV3d9UOHJmWK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "enCoSjtZNd7a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    }
  ]
}